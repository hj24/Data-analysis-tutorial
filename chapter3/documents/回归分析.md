# Chapter 3挖掘建模

## S4.回归分析

### 简介

1. 回归分析的大致研究范围：

   ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuh7hnnijjj30fn0dvjtr.jpg)

2. 常见的回归模型：

   1. 线性回归：对一个或多个自变量之间的线性关系进行建模，可用最小二乘法求系数
   2. 非线性回归：对一个或多个自变量间的非线性关系进行建模，如果线性关系可以转换为线性关系，则用线性回归的思想求解，如果不能，则用非线性最小二乘法求解
   3. logistic回归：广义线性模型的特例，利用ligistic函数将因变量取值范围控制在0，1之间，表示取值为1的概率
   4. 岭回归：对最小二乘法的改进，避免多重共线性
   5. 主成分回归：根据主成分分析的思想提出，是对最小二乘法的一种改进

3. 本篇主要总结：线性回归，多项式回归，岭回归，logistic回归

### 实例

- 线性回归，多项式回归，岭回归：代码见`Regression.py`

- 代码开头`import numpy`

  ```python
  #!/usr/bin/env python
  # -*- coding: utf-8 -*-
  
  import numpy as np
  ```


1. 最小二乘线性回归

   ```python
   # 线性回归
   # 最小二乘法
   from sklearn.linear_model import LinearRegression
   reg_model = LinearRegression()
   x = np.mat([[0,0],[1,1],[2,2]])
   y = np.mat([[0],[1],[2]])
   reg_model.fit(x,y)
   print('自变量系数：{} 截距：{}'.format(reg_model.coef_,reg_model.intercept_))
   ```

   - 从`sklearn`的线性模块中导入`LinearRegression()`

   - 回归方程形式如下：

     ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuh8275nsng304k00f0fu.gif)

   - 用`reg_model.coef_,reg_model.intercept_`分别取得系数和截距

   **运行结果**

   ```
   自变量系数：[[ 0.5  0.5]] 截距：[  1.11022302e-16]
   ```

   最小二乘法的缺陷：对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 ![X](http://sklearn.apachecn.org/cn/0.19.0/_images/math/7a7bb470119808e2db2879fc2b2526f467b7a40b.png)的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性的情况可能真的会出现。

2. 岭回归：对最小二乘法的改进

   ```python
   # 岭回归避免多重共线问题
   from sklearn.linear_model import Ridge
   x_1 = np.mat([[0,0],[0,0],[1,1]])
   y_1 = np.mat([[0],[.1],[1]])
   reg_model = Ridge(alpha = .5,fit_intercept=True)
   reg_model.fit(x_1,y_1)
   print('自变量系数：{} 截距：{}'.format(reg_model.coef_,reg_model.intercept_))
   ```

   - 同样从`sklearn`的线性模块中导入`Ridge()`
   - 岭回归通过对系数的大小施加惩罚来解决普通最小二乘法的一些问题
   - 其中，alpha是控制系数收缩量的复杂性参数： alpha的值越大，收缩量越大，这样系数对共线性的鲁棒性也更强。

   **运行结果**

   ```
   自变量系数：[[ 0.34545455  0.34545455]] 截距：[ 0.13636364]
   ```

3. `RidgeCV`通过内置的 Alpha 参数的交叉验证来实现岭回归 

   ```python
   # 通过内置的 Alpha 参数的交叉验证来实现岭回归
   from sklearn.linear_model import RidgeCV
   x_2 = np.mat([[0,0],[0,0],[1,1]])
   y_2 = np.mat([[0],[.1],[1]])
   reg_model = RidgeCV(alphas=[0.1,1.0,10.0],fit_intercept=True)
   reg_model.fit(x_2,y_2)
   print('自变量系数：{} 截距：{}'.format(reg_model.coef_,reg_model.intercept_))
   print('最适参数alpha：{}'.format(reg_model.alpha_))
   print('x1=1,x2=2时，函数值为：{}'.format(reg_model.predict([[1,1]])))
   print('拟合函数的得分：{}'.format(reg_model.score(x_2,y_2)))
   ```

   - 通过设置alpha的值为0.1，1.0，10.0来交叉验证，拟合函数
   - `.alpha_`可以得到最适合的alpha值
   - `.score()`可以得到拟合函数的评价值，`.predict()`可以预测新值

   **运行结果**

   ```
   自变量系数：[[ 0.44186047  0.44186047]] 截距：[ 0.07209302]
   最适参数alpha：0.1
   x1=1,x2=2时，函数值为：[[ 0.95581395]]
   拟合函数的得分：0.9869308625392984
   ```

4. 多项式回归

   ```python
   # 多项式回归
   from sklearn.preprocessing import PolynomialFeatures
   x = np.arange(6).reshape(3,2)
   print('x: \n{}'.format(x))
   # 把x转化为最高项为2次的多项式形式
   poly = PolynomialFeatures(degree=2)
   x = poly.fit_transform(x)
   print('转变之后x:\n{}'.format(x))
   # 通过Pipeline工具来简化
   from sklearn.pipeline import Pipeline
   model = Pipeline([('poly',PolynomialFeatures(degree=3)),
   	('linear',LinearRegression(fit_intercept=False))])
   x_1 = np.arange(5)
   y = 3 - 2*x_1 + x_1**2 - x_1**3
   model = model.fit(x_1[:,np.newaxis],y)
   print('多项式回归系数：{}'.format(model.named_steps['linear'].coef_))
   ```

   - 多项式回归的本质思想是用线性模型拟合非线性函数

   - 可以通过构造系数的 **polynomial features** 来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型:

     ![](https://ws3.sinaimg.cn/large/006tNc79ly1fz0r3n9hvmg306000i0lk.gif)

     如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:

     ![](https://ws4.sinaimg.cn/large/006tNc79ly1fz0r4ap9uxg30bg00ma9t.gif)

     观察到这 *还是一个线性模型* : 看到这个，想象创造一个新的变量

     ![](https://ws4.sinaimg.cn/large/006tNc79ly1fz0r4wrss3g304y00m0iq.gif)

     有了这些重新标记的数据，我们可以将问题写成

     ![](https://ws1.sinaimg.cn/large/006tNc79ly1fz0r5g9tyzg30ap00i741.gif)

     我们看到，所得的 *polynomial regression* 与我们上文所述线性模型是同一类，因此可以用同样的方法解决。通过用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。

   - `x = np.arange(6).reshape(3,2)`生成0-5六个数，并把它reshape成3行2列的矩阵

   - `PolynomialFeatures(degree=2)`把x转化为最高项为2次的多项式

     ```python
     x = np.arange(6).reshape(3,2)
     print('x: \n{}'.format(x))
     # 把x转化为最高项为2次的多项式形式
     poly = PolynomialFeatures(degree=2)
     x = poly.fit_transform(x)
     print('转变之后x:\n{}'.format(x))
     ```

     **运行结果**

     ```
     x: 
     [[0 1]
      [2 3]
      [4 5]]
     转变之后x:
     [[  1.   0.   1.   0.   0.   1.]
      [  1.   2.   3.   4.   6.   9.]
      [  1.   4.   5.  16.  20.  25.]]
     ```

     - 我们用`Pipeline`工具来集成多项式回归的模型，并用它来检验`y = 3 - 2*x_1 + x_1**2 - x_1**3`这个函数

     ```python
     # 通过Pipeline工具来简化
     from sklearn.pipeline import Pipeline
     model = Pipeline([('poly',PolynomialFeatures(degree=3)),
     	('linear',LinearRegression(fit_intercept=False))])
     x_1 = np.arange(5)
     y = 3 - 2*x_1 + x_1**2 - x_1**3
     model = model.fit(x_1[:,np.newaxis],y)
     print('多项式回归系数：{}'.format(model.named_steps['linear'].coef_))
     ```

     - np.newaxis可以在矩阵中新添加1个纬度

     **运行结果**

     ```
     多项式回归系数：[ 3. -2.  1. -1.]
     ```

- logistic回归：代码见：`LogisticRegression.py`

- 预备代码，必备库和读取数据的接口

  ```python
  #!/usr/bin/env python
  # -*- coding: utf-8 -*-
  
  import os 
  import pandas as pd
  
  def get_data(data_name='',index_col=''):
  	current_path = os.getcwd()
  	parent_path = os.path.dirname(current_path)
  	data_dir = os.path.join(parent_path,'data')
  	for f in os.listdir(data_dir):
  		if f.endswith(data_name):
  			data_path = os.path.join(data_dir,f)
  	if index_col == '':
  		data = pd.read_excel(data_path)
  	else:
  		data = pd.read_excel(data_path,index_col=index_col)
  	return data,os.path.dirname(data_path)
  ```

1. logistic回归介绍

   logistic回归模型中的因变量只有1-0（如是和否，发生和不发生），利用ligistic函数将因变量取值范围控制在0，1之间，表示取值为1的概率

   [吴恩达机器学习课程](https://www.bilibili.com/video/av9912938?from=search&seid=13582254560329413929)中logistic回归一节讲的很清楚，可以戳这个链接去看

2. logistic回归建模步骤

   ![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuha85o78kj306h079dgk.jpg)

3. 实例

   以一个银行贷款拖欠率数据为例，进行logistic回归，来对是否违约进行分类

   数据集样式：

   ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuhaf0cv94j30lp0850tw.jpg)

   0. 读取数据

      ```python
      if __name__ == '__main__':
      	data,data_path = get_data('bankloan.xls',index_col='')
      	x = data.iloc[:,:8].as_matrix()
      	y = data.iloc[:,8].as_matrix()
      ```

   1. 按步骤，首先要筛选特征，因为这里前7列作为特征，纬度有点大，而筛选特征的方法有很多种，简单的有F检验，给出各个特征的F值和P值，从而选择F值大的，P值小的。稍微新一点的方法有递归消除特征(RFE)和稳定性选择方法。

      这里我们使用稳定性选择的方法

      ```python
      	from sklearn.linear_model import LogisticRegression as LR
      	from sklearn.linear_model import RandomizedLogisticRegression as RLR
      	# 建立随机逻辑回归模型
      	rlr = RLR()
      	rlr.fit(x,y)
      	print('获取特征筛选结果，也可以用.score_来获取：\n{}'.format(rlr.get_support()))
          print('通过随机逻辑回归模型筛选特征结束, 有效特征为：')
      	for item in data.columns[:8][rlr.get_support()]:
      		print('{}'.format(item))
      	x = data[data.columns[:8][rlr.get_support()]].as_matrix()
      ```

      - 用`.get_support()`方法获取筛选特征的结果

      **运行结果**

      ```
      获取特征筛选结果，也可以用.score_来获取：
      [False False  True  True False  True  True False]
      通过随机逻辑回归模型筛选特征结束, 有效特征为：
      工龄
      地址
      负债率
      信用卡负债
      ```

      - 训练

        ```python
        lr = LR()
        lr.fit(x,y)
        print('逻辑回训练结束')
        print('模型的平均正确率为：{}'.format(lr.score(x,y)))
        # 可以用lr.predict()来进进行预测
        ```

      **运行结果**

      ```
      模型的平均正确率为：0.8142857142857143
      ```
