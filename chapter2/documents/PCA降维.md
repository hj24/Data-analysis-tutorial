# Chapter 2数据预处理

## S1.PCA降维

### 前言

1. 本节重点介绍PCA降维算法以及它在sklearn中如何使用
2. 文章分为三部分，第一部分讲原理，第二部分讲基本操作，第三部分以一个实例应用

### 算法原理

- [知乎一篇回答](https://www.zhihu.com/question/38417101)
- [英文原文教程](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)

1. 前言

   pca是现在很常见的降维算法，然而现在很多人都只知道如何用他，缺很少有理解它的算法原理的，这篇的前半部分，我们尝试用通俗易懂的方法解释pca的原理，上面两个链接是本篇文章的参考资料，感兴趣可以戳进去看一看。

2. 正文

   想象你正在一个家庭聚会上。许久不见的奶奶和你聊天：

   **奶奶: **听说你最近在研究什么pca啊，可以和奶奶讲讲这是个啥嘛？

   **你: **pca其实就是一个浓缩数据的方法啦，奶奶你看啊，咱桌上这几瓶葡萄酒，我们要区别它的好坏啊，可以用颜色啊，年代啊，酒精浓度啊这些指标来区分它，这几瓶酒还是简单的，如果我带你去酒窖里参观一下，那里那么多酒，相应的，区分的特征就多了啊，很多都是相似的，如果我们要辨别好坏，那就很麻烦了，比如酒颜色越深时，往往浓度也越高，那我们就可以把这两个特征合二为一，这样就可以用比较少的特征来概括描述咱们的酒了。这个就是PCA做的事啦。

   **奶奶: **为什么要做浓缩呢，用原来的东西不是蛮好的嘛...

   **你: **这就跟浓缩洗衣粉是一个道理啊，我每次洗衣服的时候，只要倒一点浓缩的洗衣粉，但是呢要是换成普通的洗衣粉，那就要倒好几勺。你看呀，洗衣粉浓缩了之后变轻了，数据嘛，浓缩了就更能节省我电脑空间啦，这样电脑处理起来就没那么费劲了。

   **奶奶: **哦哦哦，奶奶懂了，pca就是检查一下哪些数据是多余的，然后把它扔掉是吧

   **你: **不不不，不过奶奶你说的也有一点道理，pca不是选出有用的特征再把多余的特征扔掉，而是根据以前的特征概括出新的特征，它可以更好的描述我们的酒，我给你举个例子，你看啊，酒的浓度减去它的酸度，就是一个新特征，它同样可以很好的告诉我们这个就怎么样，但是，以前的两个特征就变成一个啦，想这样的好几个特征的组合还有好多，pca做的就是找出那些最有用的组合(这里实际上就是线性组合)

   **妈妈: **这听起来有点意思啊，但是我还是不理解你说的新特征概括就特征是什么意思...

   **你: **首先呢，我们描述酒的时候肯定想要找一些区分度大的特征啊，家里这几瓶酒，价格都差不多，我们光看价格看不出它的好坏，这就说明价格这个特征没有起到概括作用，所以呢肯定要再去看其他区分度大的特征啊，pca要做的就是找到这些可以概括这瓶酒的新特征。

   其次呢，我们也想要找到这样的特征：它能让我们重新看到酒的原貌，我们看完那些高度概括的比如浓度减酸度的特征之后，知道了这是一瓶好酒，那我们肯定还要看看它原来的特征呀，比如说我们还得再看酸度，看看这瓶酒口感怎么样，再举个例子，一张图打码之后，我们还是希望以后可以把它变回原来的高清大图的。对，就是想找到这种超能力的新特征。

   巧的是，这两个目标其实是一样的，而且pca都能够实现它们哟。

3. 结语

   以上一个简单的情景，大致可以讲明白pca做了哪些是，以及为什么要做这些事，想要更深一点的了解，可以戳开头的那两个链接，下面我们要做的就是去学一学在sklearn中如何去使用pca。

### pca在sklearn中的实现

- 代码见chapter2中的`learnPCA.py`
- 本代码包含两个小测试，一个是在自己生成的数据上，一个是在实际的数据集上

1. 导入相关的库

   ```python
   #!/usr/bin/env python
   # -*- coding: utf-8 -*-
   
   import os 
   import pandas as pd
   from sklearn.decomposition import PCA
   import numpy as np
   ```

   - python中pca的实现主要在sklearn这个机器学习库中，通过`from sklearn.decomposition import PCA`将它导入

2. 生成自己的数据，用于测试pca

   ```python
   def test_1():
   	data = np.random.rand(10,4)
   	print('数据概况：\n{}'.format(data))
   	pca = PCA(n_components=2)
   	pca.fit(data)
   	print('特征向量：\n{}'.format(pca.components_))
   	print('各属性贡献率：\n{}'.format(pca.explained_variance_ratio_))
   ```

   1. `np.random.rand(10,4)`产生一个随机生成的十行四列的矩阵
   2. `pca = PCA(n_components=2)`调用pca方法，通过设置`n_components`参数设置要转化成的维度
   3. `.fit(data)`方法训练数据，`.components_`参数查看降维后的新特征，`.explained_variance_ratio_`查看这些特征的贡献率

   在mian中我们来执行一下

   ```python
   if __name__ == '__main__':
   	choice = True
   	if choice:
   		test_1()
   	else:
   		test_2()
   ```

   运行结果：

   ```
   数据概况：
   [[ 0.16745351  0.77272164  0.42240346  0.26821385]
    [ 0.56976229  0.34203901  0.87004722  0.11280409]
    [ 0.9104358   0.83139167  0.37057394  0.77268875]
    [ 0.34859503  0.95854874  0.39702929  0.29479228]
    [ 0.75691921  0.62451074  0.66982589  0.85338744]
    [ 0.62963178  0.20458949  0.05432838  0.94935112]
    [ 0.82251906  0.27361193  0.23630615  0.64300136]
    [ 0.70492741  0.60231343  0.44963647  0.91851176]
    [ 0.7255887   0.02126886  0.17309954  0.24227527]
    [ 0.19002972  0.15993575  0.96417476  0.82192234]]
   特征向量：
   [[ 0.52379872 -0.3078128  -0.46462923  0.64420949]
    [-0.11336679 -0.94928469  0.17903223 -0.23228002]]
   各属性贡献率：
   [ 0.37195354  0.27588935]
   ```

   - 可以看到我们原来特征是每一个行向量，转化之后只剩两个行向量也就是降成了2维的
   - 贡献率可以看成是它们每个向量的概括能力

3. 将2018苏北建模比赛的b题数据用于测试pca

   ```python
   def test_2():
   	data = pd.read_excel('/Users/macbook/Desktop/Data-analysis-tutorial/chapter2/data/2018-51MCM-Appendix B.xlsx',
   		index_col='指标名称',
   		sheet_name=0,
   		skiprows=[0])
   	print('数据概况：\n{}'.format(data.describe()))
   	data_train = data.iloc[:32,:20]
   	
   	for i in range(data_train.shape[0]):
   		for j in range(data_train.shape[1]):
   			if data_train.iloc[i,j] == '——':
   				data_train.iloc[i,j] = np.nan
   	data_train = data_train.fillna(axis=0,method='ffill')	
   	print('填补缺失值后: \n{}'.format(data_train))
   	pca = PCA(n_components=3)
   	pca.fit(data_train.T)
   	print('特征向量：\n{}'.format(pca.components_))
   	print('各属性贡献率：\n{}'.format(pca.explained_variance_ratio_))
   ```

   数据预览：

   ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fun52b3llsj30pu0bhjtv.jpg)

   方法和上面一样：

   1. 先读取数据
   2. `pca = PCA(n_components=3)`调用pca方法，通过设置`n_components`参数设置要转化成的维度
   3. `.fit(data_train.T)`方法训练数据，`.components_`参数查看降维后的新特征，`.explained_variance_ratio_`查看这些特征的贡献率

   运行结果：

   ```
   特征向量：
   [[ 0.09788311  0.10340709  0.10753801  0.1101786   0.1154865   0.12064005
      0.12181638  0.12415874  0.12992134  0.13546519  0.13800862  0.14075569
      0.1502278   0.15480277  0.15814222  0.16012968  0.16736686  0.17427828
      0.17282041  0.17467294  0.19158185  0.20223236  0.20515423  0.20817383
      0.2164823   0.2243388   0.227839    0.23100646  0.23877235  0.24493152
      0.24893887  0.2517441 ]
    [ 0.22217646  0.2177108   0.21165345  0.23507608  0.25455042  0.2579151
      0.2612193   0.24635939  0.23661754  0.20570123  0.17837349  0.17191712
      0.15506678  0.11795339  0.10637192  0.10130717  0.08051618  0.04743173
      0.04300604  0.01374289 -0.06963258 -0.09886853 -0.12944328 -0.16169098
     -0.19027223 -0.17893548 -0.18134166 -0.15451202 -0.15946085 -0.19108372
     -0.18288944 -0.18725139]
    [-0.43444893 -0.38960028 -0.33713697 -0.21490254 -0.06042615  0.0300574
      0.10330701  0.10876861  0.20677756  0.2063527   0.10692088  0.12528957
      0.0851219   0.02710438  0.08466617  0.10110033  0.12283225  0.11959818
      0.10473991  0.04263295 -0.07042885  0.00227974 -0.10477301 -0.19424667
     -0.3034464  -0.2039998  -0.14774356  0.00510174  0.11772466  0.09369741
      0.16538518  0.18256825]]
   各属性贡献率：
   [  9.99999998e-01   2.24682239e-09   1.11450245e-10]
   [Finished in 1.9s]
   ```

   - 运行结果的数据概况就不放上来了，大家可以在`chapter2/data/2018-51MCM-Appendix B.xlsx`里看到
   - 和上一题不同的是，我们训练的是`data_train.T`，`.T`实际上就是矩阵的转置，因为我们要降维的特征向量都是列向量

   